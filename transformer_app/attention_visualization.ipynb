{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules.TransformerModule import TransformerModule\n",
    "from config.core import config\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_vocab_size': 1024, 'tgt_vocab_size': 100, 'src_seq_len': 350, 'tgt_seq_len': 500, 'seq_len': 350, 'lang_src': 'en', 'lang_tgt': 'it', 'd_model': 512, 'num_layer': 0, 'num_neads': 8, 'dropout': 0.0, 'd_ff': 256, 'lr': 0.0001, 'batch_size': 16, 'epochs': 20, 'ckpt_file': 'None', 'tokenizer_file': 'tokenizer_{0}.json'}\n"
     ]
    }
   ],
   "source": [
    "print(config.model_transformer.TR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS SIZE: 32332\n",
      "Max lengh of src seq: 309\n",
      "Max lengh of src tgt: 309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModule(\n",
       "  (src_embeded): InputEmbeddings(\n",
       "    (embedding): Embedding(15698, 512)\n",
       "  )\n",
       "  (tgt_embeded): InputEmbeddings(\n",
       "    (embedding): Embedding(22463, 512)\n",
       "  )\n",
       "  (src_pos_enc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (tgt_pos_enc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder_blocks): ModuleList()\n",
       "  (decoder_blocks): ModuleList()\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList()\n",
       "    (norm_layer): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList()\n",
       "    (norm_layer): LayerNormalization()\n",
       "  )\n",
       "  (proj_layer): ProjectionLayer(\n",
       "    (proj_layer): Linear(in_features=512, out_features=22463, bias=True)\n",
       "  )\n",
       "  (transformer_model): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (layers): ModuleList()\n",
       "      (norm_layer): LayerNormalization()\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList()\n",
       "      (norm_layer): LayerNormalization()\n",
       "    )\n",
       "    (src_embed): InputEmbeddings(\n",
       "      (embedding): Embedding(15698, 512)\n",
       "    )\n",
       "    (tgt_embed): InputEmbeddings(\n",
       "      (embedding): Embedding(22463, 512)\n",
       "    )\n",
       "    (src_pos): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (tgt_pos): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (proj_layer): ProjectionLayer(\n",
       "      (proj_layer): Linear(in_features=512, out_features=22463, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from pathlib import Path\n",
    "model_ckpt_file = Path(\n",
    "    r\"D:\\ML_AI_DL_Projects\\projects_repo\\transformer\\test_dir\\test_01\\version_1\\checkpoints\\epoch=19-step=36380.ckpt\"\n",
    ")\n",
    "\n",
    "model = TransformerModule.load_from_checkpoint(model_ckpt_file)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = model.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_batch():\n",
    "    # Load a sample batch from the validation set\n",
    "    batch = next(iter(val_ds))\n",
    "    encoder_input = batch[\"encoder_input\"]\n",
    "    encoder_mask = batch[\"encoder_mask\"]\n",
    "    decoder_input = batch[\"decoder_input\"]\n",
    "    decoder_mask = batch[\"decoder_mask\"]\n",
    "    print(f\"encoder_input:{encoder_input.shape}\")\n",
    "    encoder_input_tokens = [model.tokenizer_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
    "    decoder_input_tokens = [model.tokenizer_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    # check that the batch size is 1\n",
    "    assert encoder_input.size(\n",
    "        0) == config.model_transformer.TR_model[\"batch_size\"], \"Batch size must be 1 for validation\"\n",
    "\n",
    "    model_out = model._greedy_decode(\n",
    "        encoder_input, encoder_mask, model.tokenizer_src, model.tokenizer_tgt, config.model_transformer.TR_model['seq_len'], device)\n",
    "    \n",
    "    return batch, encoder_input_tokens, decoder_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data, takeover fun\n",
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threee attention encoder, decoder,cross-attn\n",
    "def get_attn_map(attn_type: str, layer: int, head: int):\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "    return attn[0, head].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize attn map/ takeover fun\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(\n",
    "        get_attn_map(attn_type, layer, head),\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
    "        .interactive()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "        charts.append(alt.hconcat(*rowCharts))\n",
    "    return alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input:torch.Size([16, 350])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dusan\\AppData\\Local\\Temp\\ipykernel_8692\\723601448.py\", line 1, in <module>\n",
      "    batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
      "  File \"C:\\Users\\dusan\\AppData\\Local\\Temp\\ipykernel_8692\\3615831309.py\", line 16, in load_next_batch\n",
      "    model_out = model._greedy_decode(\n",
      "  File \"D:\\ML_AI_DL_Projects\\projects_repo\\transformer\\transformer_app\\modules\\TransformerModule.py\", line 192, in _greedy_decode\n",
      "    # Precompute the encoder output and reuse it for every step\n",
      "  File \"D:\\ML_AI_DL_Projects\\projects_repo\\transformer\\transformer_app\\models\\Transformer.py\", line 32, in encode\n",
      "    x = self.src_embed(x)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"D:\\ML_AI_DL_Projects\\projects_repo\\transformer\\transformer_app\\models\\InputEmbeddings.py\", line 23, in forward\n",
      "    return self.embedding(x) * math.sqrt(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\torch\\nn\\functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"D:\\envs\\pytorch_113_gpu\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "# print(f'Source: {batch[0][\"src_text\"][0]}')\n",
    "# print(f'Target: {batch[0][\"tgt_text\"][0]}')\n",
    "# sentence_len = encoder_input_tokens.index(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "418ddc00673675793dc5b636b2b189fd95ed189e6d0facbf2c55eedc0486d0d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
